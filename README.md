# JusticeGraph - Intelligent Judicial Analytics Platform

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Phase](https://img.shields.io/badge/Phase-2%20Complete-success)](https://github.com/RudranshKaran/justicegraph)

JusticeGraph is a comprehensive data-driven platform designed to assist the Indian judicial system in reducing case backlogs, optimizing hearing schedules, and improving transparency through data science, machine learning, and intelligent optimization.

## ğŸ¯ Project Vision

Transform judicial operations through:
- **Automated Data Collection** - Scrape and process judicial data from public sources
- **Intelligent Analytics** - Identify bottlenecks, trends, and performance metrics
- **AI-Powered Prioritization** - Rank cases by urgency using ML models
- **Optimized Scheduling** - Generate efficient hearing calendars with constraint programming
- **Predictive Insights** - Forecast case durations and resource needs

## ğŸ“Š Project Status

| Phase | Status | Description |
|-------|--------|-------------|
| **Phase 1** | âœ… Complete | Data collection, parsing, and ETL pipeline |
| **Phase 2** | âœ… Complete | EDA, ML models, prioritization, and scheduling optimization |
| **Phase 3** | ğŸ”„ Planned | Web dashboard, real-time analytics, API development |

---

## ğŸš€ Quick Start

### For Phase 2 (Analytics & Optimization)

```bash
# Install dependencies
pip install -r requirements.txt

# Start exploring with Jupyter
jupyter notebook analysis/eda_overview.ipynb

# Or run quick analytics
python -c "from modeling.priority_model import calculate_priority_scores; df = calculate_priority_scores(); print(f'Prioritized {len(df)} cases')"
```

**ğŸ“˜ See [PHASE2_QUICKSTART.md](PHASE2_QUICKSTART.md) for detailed Phase 2 guide**

---

## ğŸ“ Project Structure

```
JusticeGraph/
â”‚
â”œâ”€â”€ ğŸ“Š analysis/                # Phase 2: Exploratory Data Analysis
â”‚   â”œâ”€â”€ case_duration_analysis.py    # Case duration metrics
â”‚   â”œâ”€â”€ backlog_trends.py            # Backlog and disposal rates
â”‚   â”œâ”€â”€ court_performance.py         # Court efficiency analysis
â”‚   â””â”€â”€ eda_overview.ipynb           # Interactive EDA notebook
â”‚
â”œâ”€â”€ ğŸ¤– modeling/                # Phase 2: ML Models
â”‚   â”œâ”€â”€ priority_model.py            # Case prioritization engine
â”‚   â”œâ”€â”€ duration_prediction.py       # ML duration forecasting
â”‚   â””â”€â”€ model_utils.py               # Feature engineering utilities
â”‚
â”œâ”€â”€ âš™ï¸ optimization/            # Phase 2: Scheduling Engine
â”‚   â”œâ”€â”€ scheduler.py                 # Intelligent hearing scheduler
â”‚   â”œâ”€â”€ constraint_builder.py        # Scheduling constraints
â”‚   â””â”€â”€ optimization_utils.py        # Validation and metrics
â”‚
â”œâ”€â”€ ğŸ“ˆ visualization/           # Phase 2: Charts & Dashboards
â”‚   â”œâ”€â”€ generate_visuals.py          # Plot generation
â”‚   â””â”€â”€ outputs/                     # Generated visualizations
â”‚
â”œâ”€â”€ ğŸ“„ reports/                 # Auto-generated reports
â”‚   â”œâ”€â”€ EDA_SUMMARY.md
â”‚   â”œâ”€â”€ PRIORITY_METRICS.md
â”‚   â”œâ”€â”€ MODEL_METRICS.md
â”‚   â””â”€â”€ SCHEDULER_RESULTS.md
â”‚
â”œâ”€â”€ ğŸ’¾ data/                    # Data storage (layered approach)
â”‚   â”œâ”€â”€ bronze/                      # Raw scraped data
â”‚   â”œâ”€â”€ silver/                      # Parsed structured data
â”‚   â””â”€â”€ gold/                        # Analysis-ready data
â”‚       â”œâ”€â”€ prioritized_cases.csv
â”‚       â”œâ”€â”€ optimized_schedule.csv
â”‚       â””â”€â”€ case_duration_analysis.csv
â”‚
â”œâ”€â”€ ğŸ—„ï¸ models/                  # Phase 1: Data Models
â”‚   â””â”€â”€ data_models.py               # SQLAlchemy ORM schemas
â”‚
â”œâ”€â”€ ğŸŒ ingest/                  # Phase 1: Web Scraping
â”‚   â”œâ”€â”€ cause_list_ingest.py
â”‚   â”œâ”€â”€ case_status_ingest.py
â”‚   â””â”€â”€ judgment_ingest.py
â”‚
â”œâ”€â”€ ğŸ”§ parse/                   # Phase 1: Data Parsing
â”‚   â”œâ”€â”€ parse_cause_list.py
â”‚   â””â”€â”€ parse_case_status.py
â”‚
â”œâ”€â”€ ğŸ”„ pipelines/               # Workflow Orchestration
â”‚   â”œâ”€â”€ phase1_pipeline.py           # Data collection pipeline
â”‚   â””â”€â”€ phase2_pipeline.py           # Analytics pipeline (TBD)
â”‚
â”œâ”€â”€ ğŸ› ï¸ utils/                   # Shared Utilities
â”‚   â”œâ”€â”€ db_utils.py                  # Database operations
â”‚   â”œâ”€â”€ logging_utils.py             # Structured logging
â”‚   â””â”€â”€ io_utils.py                  # File I/O helpers
â”‚
â”œâ”€â”€ âš™ï¸ configs/                 # Configuration
â”‚   â”œâ”€â”€ sources.yaml                 # Data source metadata
â”‚   â””â”€â”€ settings.env                 # Environment variables
â”‚
â”œâ”€â”€ ğŸ“š documentation/           # Technical Documentation
â”‚   â”œâ”€â”€ DATA_DICTIONARY.md
â”‚   â”œâ”€â”€ PIPELINE_OVERVIEW.md
â”‚   â””â”€â”€ MODEL_DESIGN.md
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ PHASE2_SUMMARY.md          # Phase 2 implementation details
â”œâ”€â”€ PHASE2_QUICKSTART.md       # Phase 2 quick start guide
â””â”€â”€ README.md                  # This file
```

4. **Configure environment**
   ```powershell
   # Copy example environment file
   cp configs/settings.env.example configs/settings.env
   
   # Edit settings.env with your database credentials
   ```

5. **Initialize database**
   ```python
   python -c "from utils.db_utils import DatabaseManager; db = DatabaseManager(); db.create_tables()"
   ```

### Configuration

Edit `configs/settings.env`:

```env
# Database Configuration
DATABASE_URL=postgresql://username:password@localhost:5432/justicegraph

# Or use SQLite for development
# DATABASE_URL=sqlite:///justicegraph.db

# Logging
LOG_LEVEL=INFO

# Scraping Configuration
REQUEST_TIMEOUT=60
RATE_LIMIT_DELAY=2.0
```

## ğŸ“š Usage Examples

### 1. Scrape Cause Lists

```python
from ingest.cause_list_ingest import CauseListScraper
from datetime import date

# Initialize scraper
scraper = CauseListScraper(
    court_code='DL-HC',
    base_url='https://delhihighcourt.nic.in'
)

# Fetch today's cause list
file_path = scraper.fetch_cause_list(date.today())
print(f"Saved to: {file_path}")
```

### 2. Parse Cause Lists

```python
from parse.parse_cause_list import CauseListParser

# Parse HTML to structured data
parser = CauseListParser()
output_path = parser.parse_and_save('data/bronze/cause_list_DL_HC_20231115.html')

# Load parsed data
import pandas as pd
df = pd.read_csv(output_path)
print(f"Parsed {len(df)} cases")
```

### 3. Normalize and Store Data

```python
from normalize.normalize_entities import normalize_case_data
from utils.db_utils import DatabaseManager

# Normalize data
normalized_df = normalize_case_data(df)

# Store in database
db = DatabaseManager()
# Insert logic here
```

### 4. Run Complete Pipeline

```python
from pipelines.phase1_pipeline import run_phase1_pipeline

# Execute full ETL workflow
run_phase1_pipeline(
    court_code='DL-HC',
    start_date='2023-11-01',
    end_date='2023-11-07'
)
```

## ğŸ”§ Data Models

### Core Entities

- **Court**: Court metadata (name, code, location, jurisdiction)
- **Judge**: Judge information (name, designation, court assignment)
- **Case**: Legal case details (number, type, parties, status, dates)
- **Hearing**: Individual hearing records (date, judge, outcome)
- **CauseList**: Daily hearing schedules
- **Judgment**: Court orders and judgments

See `documentation/DATA_DICTIONARY.md` for complete field descriptions.

## ğŸ“Š Data Sources

### Currently Supported

- **eCourts Services Portal**: Case status and cause lists
- **High Court Websites**: Delhi HC, Bombay HC, etc. (customizable)
- **NJDG (National Judicial Data Grid)**: Aggregate statistics

### Planned

- **Supreme Court of India**: SCI case database
- **IndianKanoon**: Judgment repository
- **District Courts**: District-level data

See `documentation/SOURCE_REGISTRY.md` for detailed source information.

## ğŸ§ª Testing

Run the test pipeline:

```powershell
python test_pipeline.py
```

Run unit tests:

```powershell
pytest tests/ -v --cov=.
```

## ğŸ“ˆ Pipeline Workflow

```
1. INGEST (Bronze Layer)
   â†“ Scrape HTML/PDF/JSON from sources
   â†“ Save with metadata and timestamps
   
2. PARSE (Silver Layer)
   â†“ Extract structured data from raw files
   â†“ Convert to DataFrames/CSV
   
3. NORMALIZE (Silver â†’ Gold)
   â†“ Clean text (remove honorifics, standardize names)
   â†“ Normalize case numbers, dates, court names
   â†“ Resolve entity references
   
4. VALIDATE
   â†“ Check for nulls, duplicates, invalid formats
   â†“ Verify referential integrity
   
5. LOAD (Gold Layer â†’ Database)
   â†“ Insert/upsert to PostgreSQL
   â†“ Update indexes and relationships
```

## ğŸ› ï¸ Development

### Code Style

- Follow PEP 8
- Use type hints
- Document with docstrings
- Use `black` for formatting

```powershell
black .
flake8 .
mypy .
```

### Adding a New Data Source

1. Create scraper in `ingest/new_source_ingest.py`
2. Create parser in `parse/parse_new_source.py`
3. Add source metadata to `configs/sources.yaml`
4. Update documentation

## ğŸ“ Logging

All operations are logged with structured JSON format:

```json
{
  "timestamp": "2023-11-15T14:30:22",
  "level": "INFO",
  "logger": "ingest",
  "message": "Scraper completed successfully",
  "scraper_name": "cause_list_ingest",
  "record_count": 150,
  "status": "success"
}
```

Logs are saved to `logs/` directory.

## ğŸ” Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | `sqlite:///justicegraph.db` |
| `LOG_LEVEL` | Logging level | `INFO` |
| `REQUEST_TIMEOUT` | HTTP request timeout (seconds) | `30` |
| `RATE_LIMIT_DELAY` | Delay between requests (seconds) | `2.0` |

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## âš ï¸ Disclaimer

This project is for **research and educational purposes only**. Always respect the terms of service of data sources and comply with applicable laws regarding web scraping and data usage.

## ğŸ¯ Roadmap

### Phase 1 (Current)
- âœ… Data collection infrastructure
- âœ… Parsing and normalization
- âœ… Database integration
- âœ… Data validation

### Phase 2 (Upcoming)
- AI-driven case prioritization
- Backlog prediction models
- Judge assignment optimization
- Interactive dashboards

### Phase 3 (Future)
- Real-time data updates
- Mobile application
- Public API
- Multi-language support

## ğŸ“ Contact

For questions or collaborations:

- **GitHub**: [@RudranshKaran](https://github.com/RudranshKaran)
- **Project**: [justicegraph](https://github.com/RudranshKaran/justicegraph)

---

**Built with â¤ï¸ to improve access to justice in India**
